## 上下文管理

1. 先获取对应模型的窗口大小(input+output)。
2. 针对不同的模型设置安全缓冲区maxAllowedTokenSize：
    * 64k预留27K缓冲区，最大窗口：64-27=37k
    * 128k预留30k缓冲区，最大窗口：128-30=98k
    * 200k预留40k缓冲区，最大窗口：200-40=160k
    * 其他：max(contextWindow - 40k, contextWindow * 0.8)
3. 上下文截断策略
    * 截断触发条件：当使用的token大于等于maxAllowedTokenSize时(接近模型窗口大小)。
    * 先尝试上下文优化：
        * 删除重复的内容读取(如文件读取)。
        * 计算优化后节省的token百分比，如果节省达到30%或更高，则终止截断。
    * 如果优化不够，则应用截断策略并通知用户：
        * 始终保留用户与AI的第一次消息对，即第一次用户发送的信息和第一次AI返回的信息。
        * 判断将要采用的截断策略类型：如果总token的一半大于maxAllowedTokenSize小，则使用更激进的保留四分之一策略，否者保留二分之一。
        * 根据上面的截断策略删除对应比例的用户与AI的消息对。
            * 二分之一截断策略：首先计算二分之一token对应多少用户-AI消息对，然后取整删除消息对。
            * 四分之一截断策略：首先计算四分之三token对应多少用户-AI消息对，然后取整删除消息对。
        * 确保最后一条被移除的消息是AI消息，保持用户-AI-用户-AI的结构。